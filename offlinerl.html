<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1.0"/>
  <title>Mildly Conservative Q-Learning in Offline RL</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>

  <!-- HEADER / NAVBAR (Matches index.html style) -->
  <header class="site-header">
    <div class="logo-and-name">
      <!-- If you have a thumbnail, uncomment and adjust: 
      <img 
        src="profile_pic.jpeg" 
        alt="Udit Ekansh Thumbnail" 
        class="site-logo"
      /> -->
      <span class="site-title">Udit Ekansh</span>
    </div>
    <nav class="main-nav">
      <ul>
        <!-- Linking back to index.html with #section anchors, so users can jump directly -->
        <li><a href="index.html#about">About</a></li>
        <li><a href="index.html#research">Research</a></li>
        <li><a href="index.html#projects">Projects</a></li>
        <li><a href="index.html#certificates">Certificates</a></li>
        <li><a href="index.html#courses">Courses</a></li>
        <li><a href="index.html#publications">Publications</a></li>
        <li><a href="index.html#misc">Misc</a></li>
        <li><a href="index.html#connect">Connect</a></li>
      </ul>
    </nav>
  </header>

  <!-- HERO / TITLE SECTION -->
  <section class="hero-section">
    <h1>Mildly Conservative Q-Learning in Offline Reinforcement Learning for Grid World Navigation</h1>
  </section>

  <!-- EXACT PROJECT CONTENT (unchanged) -->
  <section style="margin-top:1rem; text-align:left; max-width:600px; margin:auto;">
    <ul>
        <p>
          Offline reinforcement learning (RL) enables policy training using pre-collected datasets, minimizing online exploration. This project evaluated offline RL algorithms for grid-world navigation, with a custom dataset generated using the A* path-planning algorithm on a 2D occupancy grid derived from a realistic environment map created in the Gazebo Simulation Environment using TurtleBot3. The Mildly Conservative Q-Learning (MCQ) algorithm and other methods, including Behavior Cloning (BC), Advantage-Weighted Regression (AWR), Batch-Constrained Q-Learning (BCQL), and Conservative Q-Learning (CQL), were implemented and compared. Results showed that MCQ and CQL failed to create policies that effectively guided the robot to goal, achieving ~0% success rates despite hyperparameter tuning using Optuna. BC achieved a 79% success rate, which improved to 94% with tuning, while AWR reached 88%. Initial results with BCQL showed promise with a 52% success rate, but its hyperparameters could not be fully tuned due to time constraints. In addition, Twin Delayed Deep Deterministic Policy Gradient + Behavior Cloning (TD3+BC) was tested in the PointMaze environment, revealing significant reward degradation when faced with out-of-distribution (OOD) actions. These results highlight the challenges of offline RL in navigation tasks and emphasize the need for robust algorithms and effective hyperparameter tuning. Future work includes validating policies on TurtleBot3 in Gazebo and addressing out-of-distribution scenarios.
          <a href="RLProject_copy.pdf" target="_blank" style="text-decoration: underline;">View Term Paper</a>
          <p> The code for this project can be found <a href="https://github.com/Udit176/OfflineRL" target="_blank" style="text-decoration: underline;">here</a>. </p>
        </p>
        <p style="text-align:center;">
          <img src="OfflineRL.png" alt="MCQL Project Image" style="max-width:100%; height:auto;">
          <br>
          <em>Source: <a href="https://sites.google.com/view/offlinerltutorial-neurips2020/home" target="_blank" style="text-decoration: underline;">Offline RL Tutorial- NeurIPS 2020</a></em>
        </p>
      </li>
    </ul>
  </section>

  <!-- FOOTER (Optional, matching style from index.html) -->
  <footer class="site-footer" style="text-align:center; margin-top:2rem;">
    <p>Â© 2024 Udit Ekansh. All rights reserved.</p>
  </footer>

</body>
</html>

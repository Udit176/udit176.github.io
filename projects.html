<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1.0" />
  <title>Udit Ekansh - Projects</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <aside class="sidebar">
    <h2>Udit Ekansh</h2>
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="resume.html">Resume</a></li>
      <li><a href="courses.html">Courses</a></li>
      <li><a href="certificates.html">Certificates</a></li>
      <li><a href="projects.html" class="active">Projects</a></li>
      <li><a href="publications.html">Publications</a></li>
      <li><a href="misc.html">Misc</a></li>
      <li><a href="contact.html">Contact</a></li>
    </ul>
  </aside>

  <div class="main-content">
    <section class="hero">
      <h1>Projects</h1>
      <div class="divider"></div>
    </section>
    <section>
<!--       <p>
        Showcase your notable personal or academic projects. Include links, code snippets, or images where relevant.
      </p> -->
      <ul style="margin-top:1rem; text-align:left; max-width:600px; margin:auto;">
        <li>
          <strong>Mildly Conservative Q-Learning in Offline Reinforcement Learning for Grid World Navigation:</strong>
          <p>
            Offline reinforcement learning (RL) enables policy training using pre-collected datasets, minimizing online exploration. This project evaluated offline RL algorithms for grid-world navigation, with a custom dataset generated using the A* path-planning algorithm on a 2D occupancy grid derived from a realistic environment map created in the Gazebo Simulation Environment using TurtleBot3. The Mildly Conservative Q-Learning (MCQ) algorithm and other methods, including Behavior Cloning (BC), Advantage-Weighted Regression (AWR), Batch-Constrained Q-Learning (BCQL), and Conservative Q-Learning (CQL), were implemented and compared. Results showed that MCQ and CQL failed to create policies that effectively guided the robot to goal, achieving ~0% success rates despite hyperparameter tuning using Optuna. BC achieved a 79% success rate, which improved to 94% with tuning, while AWR reached 88%. Initial results with BCQL showed promise with a 52% success rate, but its hyperparameters could not be fully tuned due to time constraints.In addition, Twin Delayed Deep Deterministic Policy Gradient + Behavior Cloning (TD3+BC) was tested in the PointMaze environment, revealing significant reward degradation when faced with out-of-distribution (OOD) actions. These results highlight the challenges of offline RL in navigation tasks and emphasize the need for robust algorithms and effective hyperparameter tuning. Future work includes validating policies on TurtleBot3 in Gazebo and addressing out-of-distribution scenarios.
             <a href="RLProject_copy.pdf" download style="color: blue; text-decoration: underline;">Download Term Paper</a>
          </p>
          <p>
            <img src="path_to_image/image.png" alt="MCQL Project Image" style="max-width:100%; height:auto;">
            <br>
            <em>Source: <a href="#">Original Source Name</a></em>
          </p>
          <p>
            <a href="#">GitHub Repository</a>
          </p>
        </li>

        <li>
          <strong>Autonomous Mapping and Navigation Using TurtleBot3:</strong>
          <p>
            Implemented algorithms for autonomous mapping using SLAM (Simultaneous Localization and Mapping) and navigation using path-planning and dynamic obstacle avoidance. The project showcases how TurtleBot3 navigates and maps an unknown environment efficiently.
          </p>
          <p>
            <a href="#">GitHub Repository</a>
          </p>
          <p>
            <strong>Demonstration Videos:</strong>
          </p>
          <ul>
            <li>
              <a href="https://www.youtube.com/watch?v=VideoID1" target="_blank">Mapping in Action</a>
            </li>
            <li>
              <a href="https://www.youtube.com/watch?v=VideoID2" target="_blank">Dynamic Obstacle Avoidance</a>
            </li>
          </ul>
        </li>
      </ul>
    </section>
  </div>
</body>
</html>
